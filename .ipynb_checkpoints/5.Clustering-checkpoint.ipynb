{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "This page describes clustering algorithms in MLlib. The guide for clustering in the RDD-based API also has relevant information about these algorithms.\n",
    "\n",
    "## K-means\n",
    "\n",
    "k-means is one of the most commonly used clustering algorithms that clusters the data points into a predifined number of clusters. The MLlib implementation includes a parallelized variant of the k-means++ method called k-means||.\n",
    "\n",
    "KMeans is implemented as an Estimator and generates a KMeansModel as the base model.\n",
    "\n",
    "### Input Columns\n",
    "\n",
    "![](5.attach_files/kmeans_inputs.png)\n",
    "\n",
    "### Output Columns\n",
    "\n",
    "![](5.attach_files/kmeans_outputs.png)\n",
    "\n",
    "### Example\n",
    "\n",
    "Refer to the Python API docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "%matplotlib inline\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"file:///usr/local/hadoop/spark/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is implemented as an Estimator that supports both EMLDAOptimizer and OnlineLDAOptimizer, and generates a LDAModel as the base model. Expert users may cast a LDAModel generated by EMLDAOptimizer to a DistributedLDAModel if needed.\n",
    "\n",
    "Refer to the Python API docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.clustering import LDA\n",
    "%matplotlib inline\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"file:///usr/local/hadoop/spark/data/mllib/sample_lda_libsvm_data.txt\")\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisecting k-means\n",
    "\n",
    "Bisecting k-means is a kind of hierarchical clustering using a divisive (or \"top-down\") approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "Bisecting K-means can often be much faster than regular K-means, but it will generally produce a different clustring.\n",
    "\n",
    "BisectingKMeans is implemented as an Estimator and generates a BisectingKMeansModel as the base model.\n",
    "\n",
    "Refer to the Python API docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "%matplotlib inline\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"file:///usr/local/hadoop/spark/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(2).setSeed(1)\n",
    "model = bkm.fit(dataset)\n",
    "\n",
    "# Evaluate clustering.\n",
    "cost = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "\n",
    "# Shows the result.\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model (GMM)\n",
    "\n",
    "A Gaussian Mixture Model represents a composite distribution whereby points are drawn from one of k Gaussian sub-distributions, each with its own probability. The spark.ml implementation uses the expectation-maximization algorithm to induce the maximum-likelihood model given a set of samples.\n",
    "\n",
    "GaussianMixture is implemented as an Estimator and generates a GaussianMixtureModel as the base model.\n",
    "\n",
    "### Input Columns\n",
    "\n",
    "![](5.attach_files/gmm_inputs.png)\n",
    "\n",
    "### Output Columns\n",
    "\n",
    "![](5.attach_files/gmm_outputs.png)\n",
    "\n",
    "### Example\n",
    "\n",
    "Refer to the Python API docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "%matplotlib inline\n",
    "\n",
    "# Loads data\n",
    "dataset = spark.read.format(\"libsvm\").load(\"file:///usr/local/hadoop/spark/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "gmm = GaussianMixture().setK(2)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians: \")\n",
    "model.gaussiansDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
